{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/getml/getml-demo/vertexai/vertexai.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "**GetML on Vertex AI**: This tutorial demonstrates how to use the `Vertex AI SDK` and `gcloud cli` to build and deploy custom containers for training and prediction of getML models.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The financial dataset from the CTU Prague Relational Learning Repository. It consists of multiple tables containing various features related to bank customers and their transaction histories. The target variable is whether a customer defaults on a loan.\n",
    "\n",
    "**Note**: This notebook is based on [Predicting the loan default risk of Czech bank customers using getML](https://github.com/getml/getml-demo/blob/master/loans.ipynb). Checkout it out first, if you want to know more about the dataset and getML in general.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal of this tutorial is to:\n",
    "\n",
    "- Train a getML model using relational data from multiple tables.\n",
    "- Save the trained model and its serialized pre-processor.\n",
    "- Build a custom getML serving container with custom prediction logic using the Custom Prediction Routine feature in the Vertex AI SDK.\n",
    "- Test the built container locally.\n",
    "- Upload and deploy the custom container to Vertex AI Predictions.\n",
    "\n",
    "**Note**: This tutorial focuses more on deploying getML models with Vertex AI than on the design of the model itself.\n",
    "\n",
    "### Costs \n",
    "\n",
    "This tutorial involves the use of billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Google Cloud Storage\n",
    "* Google Container Registry\n",
    "\n",
    "\n",
    "**TIP**: Check out [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you begin\n",
    "\n",
    "**Note**: If you are running this notebook on **Vertex AI Workbench**, your environment already meets most requirements.\n",
    "However, you need to add the `storage.admin` role to the `*compute@developer.gserviceaccount.com` **service account** that is assigned to this notebook by default. Please be aware of step `VertexAI Workbench: Adding Role to Service Account`\n",
    "\n",
    "If you run this notebook locally, please consider the following requirements:\n",
    "\n",
    "### Set up Your Local Development Environment\n",
    "\n",
    "If you run this notebook on your local machine make sure your environment meets this notebook's requirements:\n",
    "\n",
    "* [Docker](https://docs.docker.com/engine/install/)\n",
    "* [Google Cloud SDK (gcloud)](https://cloud.google.com/sdk/docs/install)\n",
    "\n",
    "**Note**: If you need to install Docker or the SDK, the links will guide you to the installation steps.\n",
    "\n",
    "### Set up Your Google Cloud Project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n",
    "\n",
    "IMPORTANT! If you have not used gcloud CLI before you need to set it up first. On your local shell, run:\n",
    "\n",
    "```bash\n",
    "gcloud init\n",
    "```\n",
    "\n",
    "During the process you will authenticate, get credentials and can set your default project / region.\n",
    "\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "**Note**: All commands prefixed with `!` are shell commands. The prefix `!` allows for direct execution within Juypter. However, you can also execute them in a dedicated Terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Environment\n",
    "\n",
    "We need to adapt to the environment this notebook runs in. So if this notebook runs on VertexAI Workbench or Colab `IS_GCLOUD_ENV` is `True`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IS_WORKBENCH_ENV = \"GOOGLE_VM_CONFIG_LOCK_FILE\" in os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install requirements\n",
    "\n",
    "\n",
    "`getml.vertexai` is located within `src`.\n",
    "This package contains:\n",
    "- `Utility functions` for accessing **GCP resources** \n",
    "- `Configurations` for this **notebook** and **training/inference containers** we will create later.\n",
    "- `Dependencies` needed for **notebook** and **docker containers**:\n",
    "    - getml==1.4.0\n",
    "    - google-cloud-aiplatform[prediction]==1.56.0\n",
    "    - pyyaml==6.0.1\n",
    "\n",
    "The **Python Cloud Client Library** [google-cloud-aiplatform](https://cloud.google.com/python/docs/reference/aiplatform/latest) is needed to interact with services from **Google Cloud**, including\n",
    "- **Vertex AI**\n",
    "- **Cloud Storage**.\n",
    "- **[prediction]** option includes **FastAPI**, that is needed for building the prediction container later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information on **getML**, checkout the [documentation](https://docs.getml.com/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install `getml.vertexai`\n",
    "\n",
    "In the Vertex AI Workbench environment, perform the following steps:\n",
    "- Download the tarball version of the `getml-demo` repository.\n",
    "- Extract the content of the project folder into the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "\n",
    "if IS_WORKBENCH_ENV:\n",
    "    # stip-components=1 is necessary to avoid creating a directory with the name of the repository\n",
    "    ! curl -L https://api.github.com/repos/getml/getml-demo/tarball/vertexai | tar --strip-components=1 -xz\n",
    "! uv pip install --force-reinstall \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kernel restart\n",
    "\n",
    "On Workbench we also need to restart the kernel to apply all changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "\n",
    "if IS_WORKBENCH_ENV:\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ On Workbench: The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefine `IS_WORKBENCH_ENV` after kernel restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IS_WORKBENCH_ENV = \"GOOGLE_VM_CONFIG_LOCK_FILE\" in os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Vertex AI SDK\n",
    "\n",
    "`aiplatform` is part of the `google-cloud-aiplatform` package. It provides a Python API for interacting with Vertex AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "Define and save configuration variables using `Config`, storing them in `config.yaml`.\n",
    "\n",
    "This method centralizes project variable definitions within the notebook and ensures availability to Docker containers created later.\n",
    "\n",
    "Access the configuration via the `cfg` instance using dot notation, e.g., `cfg.REGION`.\n",
    "\n",
    "**Note**: If you are using Vertex AI Workbench, this notebook is associated with the **Compute Engine default service account**. `SERVICE_ACCOUNT_NAME` will be automatically filled with the corresponding *compute@developer.gserviceaccount.com name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getml.vertexai.config import Config\n",
    "\n",
    "cfg = Config(\n",
    "    {\n",
    "        \"GCP_PROJECT_NAME\": \"\",  # NOTE: Must be globally(!) unique on GCP\n",
    "        \"BUCKET_NAME\": \"\",  # NOTE: Must be globally(!) unique on GCP\n",
    "        \"BUCKET_DIR_MODEL\": \"model_artifact\",\n",
    "        \"BUCKET_DIR_DATASET\": \"datasets\",\n",
    "        \"REGION\": \"europe-west1\",  # NOTE: Adapt to your preferred region\n",
    "        \"SERVICE_ACCOUNT_NAME\": \"getml-vertexai-sa\",  # NOTE: Gets replaced, if you run on Vertex AI Workbench\n",
    "        \"DOCKER_REPOSITORY\": \"getml-vertexai-docker-repository\",\n",
    "        \"GETML_PROJECT_NAME\": \"Loans\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save configuration for later use in Docker containers\n",
    "cfg.save(\"config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print all available configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the **project** and **region** to ensure `! gcloud` commands are executed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud config set project {cfg.GCP_PROJECT_NAME}\n",
    "! gcloud config set ai/region {cfg.REGION}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Initialize the Vertex AI SDK and set the project and location defaults there as well.\n",
    "This ensures all `aiplatform` related commands/functions execute on the correct project and region.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=cfg.GCP_PROJECT_NAME, location=cfg.REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Necessary APIs\n",
    "\n",
    "If you have just created a new project, some APIs might not be enabled yet. Use the following command to enable all the APIs needed for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud services enable \\\n",
    "    iam.googleapis.com \\\n",
    "    compute.googleapis.com \\\n",
    "    containerregistry.googleapis.com \\\n",
    "    aiplatform.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Service Account\n",
    "\n",
    "We need a service account to provide our containers appropriate permissions to access\n",
    "\n",
    "* **Storage Buckets** (Save and load model artifacts)\n",
    "* **MetadataStore** (Logging metrics/Experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **VertexAI Workbench:** Adding Role to Service Account\n",
    "\n",
    "If you are running this notebook on **Vertex AI Workbench**, it is associated with a **Service Account** (see `SERVICE_ACCOUNT_EMAIL`). To ensure proper functionality, you need to add the `storage.admin` role to this account.\n",
    "\n",
    "Perform this step on the GCP Platform. Follow the link below (it should automatically open) and add the `storage.admin` role to the **Service Account** associated with this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getml.vertexai import open_iam_permissions\n",
    "\n",
    "if IS_WORKBENCH_ENV:\n",
    "    open_iam_permissions(cfg.GCP_PROJECT_NAME)\n",
    "\n",
    "    cfg.print([\"SERVICE_ACCOUNT_EMAIL\"])\n",
    "    cfg.print_links([\"iam_permissions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Local Environment:** Create a Service Account\n",
    "\n",
    "If this notebook runs on a **local environment** and you are authenticated to `gcloud cli` with your personal account, we need to create a **service account**.\n",
    "\n",
    "NOTE: If you run this notebook on **VertexAI Workbench** skip this step and continue with `Save Service Account to JSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If the service account already exists in the project, the following error can be ignored:\n",
    "# ERROR: (gcloud.iam.service-accounts.create) Resource in projects [$PROJECT_ID] is the subject of a conflict..\n",
    "\n",
    "if not IS_WORKBENCH_ENV:\n",
    "    cfg.print([\"SERVICE_ACCOUNT_NAME\"])\n",
    "\n",
    "    ! gcloud iam service-accounts create {cfg.SERVICE_ACCOUNT_NAME} \\\n",
    "        --display-name=\"getML Vertex AI Service Account\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set Permissions on Service Account\n",
    "\n",
    "Once the **service account** is created, we need to grant the roles `aiplatform.user` and `storage.admin` to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not IS_WORKBENCH_ENV:\n",
    "    cfg.print([\"GCP_PROJECT_NAME\", \"SERVICE_ACCOUNT_EMAIL\"])\n",
    "\n",
    "    # Assign the Vertex AI User role\n",
    "    ! gcloud projects add-iam-policy-binding {cfg.GCP_PROJECT_NAME} \\\n",
    "        --member=\"serviceAccount:{cfg.SERVICE_ACCOUNT_EMAIL}\" \\\n",
    "        --role=\"roles/aiplatform.user\"\n",
    "\n",
    "    # Assign the Storage Admin role\n",
    "    ! gcloud projects add-iam-policy-binding {cfg.GCP_PROJECT_NAME} \\\n",
    "        --member=\"serviceAccount:{cfg.SERVICE_ACCOUNT_EMAIL}\" \\\n",
    "        --role=\"roles/storage.admin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Service Account to JSON\n",
    "\n",
    "We will need the `service_account.json` file later when we create a **local endpoint** to test our container.\n",
    "\n",
    "\n",
    "NOTE: If too many keys have been created, the following error can occur:\n",
    "\n",
    "`ERROR: (gcloud.iam.service-accounts.keys.create) FAILED_PRECONDITION: Precondition check failed.`\n",
    "\n",
    "In this case older keys should be deleted before creating a new one.\n",
    "\n",
    "To prevent this from happening in the first place, we check if a service_account.json is already present before we create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type: ignore\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "cfg.print([\"SERVICE_ACCOUNT_EMAIL\"])\n",
    "\n",
    "PATH_SERVICE_ACCOUNT_CREDENTIALS = Path(\"service_account.json\")\n",
    "\n",
    "if not PATH_SERVICE_ACCOUNT_CREDENTIALS.exists():\n",
    "    ! gcloud iam service-accounts keys create {PATH_SERVICE_ACCOUNT_CREDENTIALS.name} \\\n",
    "        --iam-account={cfg.SERVICE_ACCOUNT_EMAIL}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cloud Storage Bucket\n",
    "\n",
    "The bucket will serve as cloud storage for:\n",
    "\n",
    "* **Trained model artifacts** (The result of the training container)\n",
    "* **Datasets** (Loans dataset)\n",
    "\n",
    "Both are included in the getML project dump, `Loans.getml`, which will be stored in the bucket we create now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If BUCKET_URI already exists. The following error can be ignored:\n",
    "# \"ServiceException 409 A Cloud Storage bucket named $BUCKET_NAME already exists.\"\n",
    "\n",
    "# Create the bucket\n",
    "! gsutil mb -l {cfg.REGION} -p {cfg.GCP_PROJECT_NAME} {cfg.BUCKET_URI}\n",
    "\n",
    "cfg.print([\"BUCKET_URI\"])\n",
    "cfg.print_links([\"bucket\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Docker Repository on Artifact Registry \n",
    "\n",
    "The **Docker repository** on Google Cloud's `Artifact Registry` will store the Docker images required for our **training** and **prediction containers**. These images will be built locally and then pushed to this repository for deployment on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: If DOCKER_REPOSITORY already exists. The following error can be ignored:\n",
    "# ERROR: (gcloud.artifacts.repositories.create) ALREADY_EXISTS: the repository already exists\n",
    "\n",
    "! gcloud artifacts repositories create {cfg.DOCKER_REPOSITORY} \\\n",
    "    --repository-format=docker \\\n",
    "    --location={cfg.REGION} \\\n",
    "    --description=\"Docker repository for getML Vertex AI Images\"\n",
    "\n",
    "cfg.print([\"DOCKER_REPOSITORY\", \"REGION\"])\n",
    "cfg.print_links([\"docker_repository\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Docker\n",
    "\n",
    "To be able to **upload images** to the **repository**, you need to update your Docker settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gcloud auth configure-docker --quiet\n",
    "! gcloud auth configure-docker --quiet {cfg.REGION}-docker.pkg.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `DOCKER_HOST` environment variable to the current docker daemon path.\n",
    "\n",
    "This is necessary for compatibility of rootless Docker setups in combination with Vertex AI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getml.vertexai.utils import get_docker_daemon_path\n",
    "\n",
    "os.environ[\"DOCKER_HOST\"] = get_docker_daemon_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling \"line buffering\" Warnings\n",
    "\n",
    "In this notebook, you may see warnings related to line buffering when using the subprocess module. These warnings do not impact the accuracy or performance and cannot be resolved within this notebook's context. Therefore, we will ignore them to keep our output clean.\n",
    "\n",
    "Note: You might still see line buffering warnings when running `! gcloud` commands. As stated, these can be safely ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"line buffering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Finished\n",
    "\n",
    "We have completed all setup and configuration steps and are now ready to start training our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This notebook demonstrates the training of a **binary classification model**. It is based on the [Loans notebook](https://github.com/getml/getml-demo/blob/master/loans.ipynb). Check out the link for more details on the dataset and usage of the getML Python API.\n",
    "\n",
    "### Main Objectives\n",
    "\n",
    "The main objectives of the training container are to:\n",
    "\n",
    "- Get and **preprocess** the `Loans dataset`.\n",
    "- `Train` a getML model (pipeline) on the **trainset**.\n",
    "- `Score` the trained model on the **testset**.\n",
    "- `Save` the project (including data and model) as an `artifact` on the `GCS Bucket`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Managed Dataset\n",
    "\n",
    "To use **experiments**, a managed dataset is essential as it creates a default **MetadataStore**. The Experiments/MetadataStore is crucial for logging and tracking experiments, ensuring all data-related activities are properly recorded and managed within the Vertex AI ecosystem.\n",
    "\n",
    "The managed dataset created here is primarily for demonstration purposes and to establish a MetadataStore. The **actual data** used to train our model is retrieved within the **training Docker container**. For details, see `training/train.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getml.vertexai import create_vertex_dataset_tabular\n",
    "\n",
    "dataset_loans = create_vertex_dataset_tabular(\n",
    "    cfg=cfg, filename_csv=\"datasets/loans_population_test.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Docker Container for Training\n",
    "\n",
    "For training we just need a simple Docker container that includes:\n",
    "\n",
    "- `Python runtime` (we conveniently use a public python image as base layer)\n",
    "- `Python dependencies`:\n",
    "    - **getml**\n",
    "    - **getml-playbooks**\n",
    "    - **google-cloud-aiplatform**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Content of Dockerfile.train:\\n\")\n",
    "%cat training/Dockerfile.train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's\n",
    "- `build` the **Dockerfile.train** image\n",
    "- and `push` it to the **Artifact Registry**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.print([\"DOCKER_IMAGE_URI_TRAIN\"])\n",
    "\n",
    "! docker build -f training/Dockerfile.train -t {cfg.DOCKER_IMAGE_URI_TRAIN} .\n",
    "! docker push {cfg.DOCKER_IMAGE_URI_TRAIN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Training Job\n",
    "\n",
    "The `gcloud ai custom-jobs create` command\n",
    "- `wraps` the **train.py** script into our Training Docker Container, then\n",
    "- `runs` it in the Vertex AI environment on **Google Cloud**.\n",
    "- Finally, the `result` is an **Artifact** containing the **getML model** and **dataframes**\n",
    "\n",
    "For more details about the command, checkout [https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create](https://cloud.google.com/sdk/gcloud/reference/ai/custom-jobs/create)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.print(\n",
    "    [\n",
    "        \"GETML_PROJECT_NAME\",\n",
    "        \"GCP_PROJECT_NAME\",\n",
    "        \"REGION\",\n",
    "        \"SERVICE_ACCOUNT_EMAIL\",\n",
    "        \"DOCKER_IMAGE_URI_TRAIN\",\n",
    "        \"BUCKET_URI_DATASET\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for the training job\n",
    "TRAIN_DISPLAY_NAME = f\"getml-train-{cfg.GETML_PROJECT_NAME}\"\n",
    "TRAIN_LOCAL_PACKAGE_PATH = \"training\"\n",
    "TRAIN_SCRIPT = \"train.py\"\n",
    "TRAIN_MACHINE_TYPE = \"n1-standard-4\"\n",
    "TRAIN_REPLICA_COUNT = 1\n",
    "\n",
    "# Create and run the custom training job\n",
    "! gcloud ai custom-jobs create \\\n",
    "  --project={cfg.GCP_PROJECT_NAME} \\\n",
    "  --region={cfg.REGION} \\\n",
    "  --display-name={TRAIN_DISPLAY_NAME} \\\n",
    "  --service-account={cfg.SERVICE_ACCOUNT_EMAIL} \\\n",
    "  --worker-pool-spec=machine-type={TRAIN_MACHINE_TYPE},replica-count={TRAIN_REPLICA_COUNT},executor-image-uri={cfg.DOCKER_IMAGE_URI_TRAIN},local-package-path={TRAIN_LOCAL_PACKAGE_PATH},script={TRAIN_SCRIPT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result of Training Container\n",
    "\n",
    "The following links contain the resources we just created, as well as the resulting artifact from the training container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.print_links([\"training_jobs\", \"model_artifact\", \"experiments\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction / Inference\n",
    "\n",
    "Now that we have a trained **Model Artifact** stored on GCS, we can\n",
    "- `build` a prediction routine that loads the **Artifact**, and\n",
    "- `deploy` an **HTTP endpoint** to run **predictions** on our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Details of the prediction container\n",
    "\n",
    "Basically, the container provides the **HTTP route** `predict` via **FastAPI / Uvicorn, Gunicorn**\n",
    "\n",
    "To know more about the `Predictor` class, see the documentation on [custom prediction routines](https://cloud.google.com/vertex-ai/docs/predictions/custom-prediction-routines).\n",
    "\n",
    "All relevant files you can find within the `prediction` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Content of Dockerfile.pred:\\n\")\n",
    "%cat prediction/Dockerfile.pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's `build` the **Dockerfile.pred** image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build -f prediction/Dockerfile.pred \\\n",
    "    -t {cfg.DOCKER_IMAGE_URI_PRED} ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Local Model\n",
    "\n",
    "Before deploying the model to the cloud, it is advisable to build and test it locally. Once the model is confirmed to be functioning correctly, you can then proceed with the cloud deployment.\n",
    "\n",
    "See the [Google documentation](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.prediction.LocalModel) for more details of the `LocalModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform.prediction import LocalModel\n",
    "\n",
    "local_model = LocalModel(serving_container_image_uri=cfg.DOCKER_IMAGE_URI_PRED)\n",
    "\n",
    "cfg.print([\"DOCKER_IMAGE_URI_PRED\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Prediction on Test Data\n",
    "\n",
    "To run a prediction using the local model, we will send a request with test data in JSON format (as string) to the local endpoint.\n",
    "\n",
    "We have prepared some test request data in JSON format, which can be loaded using `load_json_from_file()`.\n",
    "\n",
    "Note: Refer to `[OPTIONAL] Create Test Request Data` for details on how this test data was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getml.vertexai import load_json_from_file\n",
    "\n",
    "request_json = load_json_from_file(\"./prediction/request_test.json\")\n",
    "request_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [OPTIONAL] Create Test Request Data\n",
    "\n",
    "If you would like to recreate the test data JSON or see how it is generated, uncomment the following code and check its source in `src/getml/vertexai/request_data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from getml.vertexai import create_test_request\n",
    "\n",
    "# create_test_request()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy `local_model` to a `local_endpoint`\n",
    "\n",
    "Now that we have the **prediction container** ready, as well as some **test data**, we can deploy a **local endpoint** and send test data to the `predict` endpoint.\n",
    "\n",
    "See the [Google documentation](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.prediction.LocalModel#google_cloud_aiplatform_prediction_LocalModel_deploy_to_local_endpoint) for more details and requirements of the `LocalModel` class and its `deploy_to_local_endpoint` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The model training and artifact creation must be completed before proceeding to the next step ⚠️</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the training has successfully finished by checking the following links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.print_links([\"training_jobs\", \"model_artifact\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Waiting for the training job to finish before proceeding to the next step.\n",
    "\n",
    "NOTE: This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getml.vertexai.utils_gcp import wait_for_training_artifact\n",
    "\n",
    "wait_for_training_artifact(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with local_model.deploy_to_local_endpoint(\n",
    "    credential_path=PATH_SERVICE_ACCOUNT_CREDENTIALS.name,\n",
    "    artifact_uri=cfg.ARTIFACT_URI,\n",
    ") as local_endpoint:\n",
    "    health_check_response = local_endpoint.run_health_check()\n",
    "    print(\n",
    "        \"Health check response:\", health_check_response, health_check_response.content\n",
    "    )\n",
    "\n",
    "    # Make a prediction\n",
    "    predict_response = local_endpoint.predict(\n",
    "        request=request_json,\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "    )\n",
    "    print(\"Predict response:\", predict_response, predict_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see an output similar to:\n",
    "\n",
    "```\n",
    "Health check response: <Response [200]> b'{}'\n",
    "Predict response: <Response [200]> b'{\"predictions\": [[0.9659892320632935], [0.8711856007575989], [0.882280170917511],... \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is an issue you can check the logs of the container build process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_endpoint.container.logs().decode(\"utf-8\").strip().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually Spin-Up Container and Call Endpoint with Test Data\n",
    "\n",
    "Alternatively, you can manually run your Docker container. This way, you have more control over the parameters of `docker run`, especially the Google environment variables.\n",
    "\n",
    "See more details about them in the [Google documentation](https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.prediction.LocalModel).\n",
    "\n",
    "**NOTE:** You should run the `docker run` command in a separate Terminal, not in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getml.vertexai import cmd_to_run_local_endpoint\n",
    "\n",
    "cmd_to_run_local_endpoint(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Push Image to GCP / Vertex AI\n",
    "\n",
    "Before we can deploy the container to the cloud, we need to push the image to the `Artifact Registry`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rebuild Prediction Container\n",
    "To ensure compatibility with GCP (x86_64), the container image must be built with the correct architecture. Regardless of your current platform, the `docker build` command will now enforce the **linux/amd64** platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build --platform linux/amd64 -f prediction/Dockerfile.pred \\\n",
    "    -t {cfg.DOCKER_IMAGE_URI_PRED} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_model.push_image()\n",
    "\n",
    "cfg.print_links([\"image_for_predictions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Model Registry\n",
    "\n",
    "The `Model Registry` serves as a centralized repository where you can manage and version your machine learning models. By uploading the model, you make it accessible for deployment and further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.print([\"GCP_PROJECT_NAME\", \"REGION\", \"ARTIFACT_URI\"])\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    project=cfg.GCP_PROJECT_NAME,\n",
    "    location=cfg.REGION,\n",
    "    local_model=local_model,\n",
    "    display_name=\"getML model (Loans)\",\n",
    "    artifact_uri=f\"{cfg.ARTIFACT_URI}\",\n",
    "    description=\"getML model trained on the Loans dataset. Generated by demo_binary_classification.ipynb\",\n",
    ")\n",
    "\n",
    "cfg.print_links([\"model_registry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Prediction Endpoint\n",
    "\n",
    "Endpoints are machine learning models made available for online prediction requests. Endpoints are useful for timely predictions from many users (for example, in response to an application request). You can also request batch predictions if you don't need immediate results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Endpoint\n",
    "\n",
    "NOTE: If you encounter a \"FailedPrecondition\" error, this is very likely related\n",
    "        to an exception thrown within the docker container.\n",
    "        You should checkout the logs of the container to find the cause.\n",
    "\n",
    "NOTE: Deployment of endpoint can take a while (30min+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_MACHINE_TYPE = \"n1-standard-4\"\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    machine_type=ENDPOINT_MACHINE_TYPE, service_account=cfg.SERVICE_ACCOUNT_EMAIL\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on Deployed Endpoint\n",
    "\n",
    "Once the endpoint is deployed, you can also make predictions using the `Test your model` feature in the Vertex AI console (see link below).\n",
    "\n",
    "As `JSON request` you can use the content of the `request_test.json` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id is just needed to build the link\n",
    "model_id = Path(endpoint.gca_resource.deployed_models[0].model).name\n",
    "cfg.print_links([\"deployed_model\"], model_id)\n",
    "\n",
    "print(\"JSON request:\", request_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_ID (int): The numerical project ID.\n",
    "# ENDPOINT_ID (int): The numerical endpoint ID.\n",
    "# Example URL format: https://europe-west1-aiplatform.googleapis.com/v1/projects/{PROJECT_ID}/locations/europe-west1/endpoints/{ENDPOINT_ID}:predict\n",
    "\n",
    "! curl -X POST \\\n",
    "    -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    \"https://{cfg.REGION}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict\" \\\n",
    "    -d \"@prediction/request_test.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should looks similar to:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"predictions\": [\n",
    "    [\n",
    "      0.96598923206329346\n",
    "    ],\n",
    "    [\n",
    "      0.87118560075759888\n",
    "    ],\n",
    "    [\n",
    "      0.882280170917511\n",
    "    ],\n",
    "\n",
    "    ...\n",
    "    \n",
    "    ],\n",
    "  \"deployedModelId\": \"5059851355955396608\",\n",
    "  \"model\": \"projects/956851751872/locations/europe-west1/models/8409526724114513920\",\n",
    "  \"modelDisplayName\": \"getML model (Loans)\",\n",
    "  \"modelVersionId\": \"1\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undeploy Endpoint\n",
    "\n",
    "Remember to undeploy your cloud endpoints after testing to avoid unnecessary costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we walked through the complete workflow of training and deploying a machine learning model using Vertex AI. We began by setting up our environment, configuring necessary project variables, and initializing Vertex AI. We then trained a binary classification model using the getML framework, logged and tracked our experiments using the MetadataStore, and saved the model artifact to Google Cloud Storage.\n",
    "\n",
    "Next, we built and tested a custom prediction routine locally before pushing our Docker image to the Artifact Registry. We deployed the trained model to the Vertex AI Model Registry and created an online prediction endpoint to serve real-time predictions. Additionally, we discussed how to manually manage the Docker container and perform batch predictions.\n",
    "\n",
    "By following these steps, you have learned how to leverage Vertex AI for end-to-end machine learning workflows, from data preprocessing and model training to deployment and prediction. This powerful combination of tools and services ensures a scalable, efficient, and well-managed approach to developing and deploying getML models on Google Cloud.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
